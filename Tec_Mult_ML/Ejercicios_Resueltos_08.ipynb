{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8a835-60c4-46ae-98db-1233897b2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "#EJERCICIO 1\n",
    "# cargar librerias-----------------------------------------------\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "# cargar dataset-------------------------------------------------\n",
    "wine = load_wine(as_frame = True)\n",
    "# ver variables predictoras wine---------------------------------\n",
    "print(wine.data.info())\n",
    "# ver variable respuesta-----------------------------------------\n",
    "print(wine.target.value_counts())\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# seleccionar todas las variables--------------------------------\n",
    "X = wine.data\n",
    "# especie de la planta\n",
    "y = wine.target\n",
    "# crear el objeto de clase arbol---------------------------------\n",
    "tree_clf = DecisionTreeClassifier(random_state = 3)\n",
    "# ajustar el arbol-----------------------------------------------\n",
    "tree_clf.fit(X, y);\n",
    "# Ojo: pip install graphviz\n",
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# definir donde vamos a guardar la figura------------------------\n",
    "ROOT_DIR = \".\"\n",
    "PATH_FIGURAS = os.path.join(ROOT_DIR, \"images\")\n",
    "os.makedirs(PATH_FIGURAS, exist_ok=True)\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file = os.path.join(PATH_FIGURAS, \"arbol_wine.dot\"),\n",
    "        feature_names = wine.feature_names,\n",
    "        class_names = wine.target_names,\n",
    "        rounded = True,\n",
    "        filled = True\n",
    "    )\n",
    "# crear el objeto de clase arbol fijando profundidad maxima a 3--\n",
    "tree_clf_b = DecisionTreeClassifier(random_state=3, max_depth=3)\n",
    "# ajustar el arbol-----------------------------------------------\n",
    "tree_clf_b.fit(X, y);\n",
    "# generar el .dot------------------------------------------------\n",
    "export_graphviz(\n",
    "        tree_clf_b,\n",
    "        out_file = os.path.join(PATH_FIGURAS, \"arbol_wine_b.dot\"),\n",
    "        feature_names = wine.feature_names,\n",
    "        class_names = wine.target_names,\n",
    "        rounded = True,\n",
    "        filled = True\n",
    "    )\n",
    "# crear el objeto de clase arbol fijando min_leaf a 4------------\n",
    "tree_clf_c = DecisionTreeClassifier(random_state=3, \n",
    "min_samples_leaf=4)\n",
    "# ajustar el arbol-----------------------------------------------\n",
    "tree_clf_c.fit(X, y);\n",
    "# generar el .dot------------------------------------------------\n",
    "export_graphviz(\n",
    "        tree_clf_c,\n",
    "        out_file = os.path.join(PATH_FIGURAS, \"arbol_wine_c.dot\"),\n",
    "        feature_names = wine.feature_names,\n",
    "        class_names = wine.target_names,\n",
    "        rounded = True,\n",
    "        filled = True\n",
    "    )\n",
    "#EJERCICIO2\n",
    "# cargar numpy---------------------------------------------------\n",
    "import numpy as np\n",
    "# definir semilla para que la particion sea la misma-------------\n",
    "np.random.seed(3)\n",
    "# definir funcion particiones------------------------------------\n",
    "def particiones(target, dataset, test_part):\n",
    "    test_part_size = int(len(dataset) * test_part)\n",
    "    mezclar_indices = np.random.permutation(len(dataset))\n",
    "    test_indices = mezclar_indices[:test_part_size]\n",
    "    train_indices = mezclar_indices[test_part_size:]\n",
    "    return dataset.iloc[train_indices], dataset.iloc[test_indices], target.iloc[train_indices], target.iloc[test_indices]\n",
    "# usar funcion particiones con test_part 0.28---------------------\n",
    "X_train, X_test, y_train, y_test = particiones(wine.target, \n",
    "wine.data, 0.281)\n",
    "\n",
    "# cargar librerias-----------------------------------------------\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# crear objeto de la clase BaggingClassifier---------------------\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state = 3), n_estimators = 500,\n",
    "    max_samples = 75, bootstrap=True, random_state = 3);\n",
    "\n",
    "# ajustar el modelo----------------------------------------------\n",
    "bag_clf.fit(X_train, y_train);\n",
    "\n",
    "# obtener estimaciones del modelo sobre la muestra de test-------\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "# comprobar resultados-------------------------------------------\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# crear objeto de la clase BaggingClassifier---------------------\n",
    "pas_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state = 3), n_estimators = 500,\n",
    "    max_samples = 75, bootstrap=False, random_state = 3);\n",
    "\n",
    "# ajustar el modelo----------------------------------------------\n",
    "pas_clf.fit(X_train, y_train);\n",
    "\n",
    "# obtener estimaciones del modelo sobre la muestra de test-------\n",
    "y_pred_pas = pas_clf.predict(X_test);\n",
    "\n",
    "# comprobar resultados-------------------------------------------\n",
    "print(accuracy_score(y_test, y_pred_pas))\n",
    "\n",
    "\n",
    "# cargar librerias-----------------------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# crear objeto de la clase RandomForestClassifier----------------\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, \n",
    "max_leaf_nodes = 4, random_state = 3, max_samples = 75);\n",
    "\n",
    "# ajustar el modelo----------------------------------------------\n",
    "rnd_clf.fit(X_train, y_train);\n",
    "\n",
    "# obtener estimaciones del modelo sobre la muestra de test-------\n",
    "y_pred_rf = rnd_clf.predict(X_test);\n",
    "\n",
    "# comprobar resultados-------------------------------------------\n",
    "print(accuracy_score(y_test, y_pred_rf))\n",
    "\n",
    "\n",
    "# comparar con los resultados de un arbol de decision------------\n",
    "tree_clf = DecisionTreeClassifier(random_state = 3, max_depth = 3);\n",
    "tree_clf.fit(X_train, y_train);\n",
    "y_pred_tree = tree_clf.predict(X_test);\n",
    "print(accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "#EJERCICIO3\n",
    "# cargar librerias-----------------------------------------------\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# crear objeto de la clase BaggingClassifier---------------------\n",
    "gb_clf = GradientBoostingClassifier(n_estimators = 500,\n",
    "random_state = 3);\n",
    "\n",
    "# ajustar el modelo----------------------------------------------\n",
    "gb_clf.fit(X_train, y_train);\n",
    "\n",
    "# obtener estimaciones del modelo sobre la muestra de test-------\n",
    "y_pred_gb = gb_clf.predict(X_test);\n",
    "\n",
    "# comprobar resultados-------------------------------------------\n",
    "print(accuracy_score(y_test, y_pred_gb))\n",
    "# crear un conjunto de posibles valores--------------------------\n",
    "learning_rates = [1, 0.5, 0.25, 0.1, 0.05, 0.01]\n",
    "max_depths = [1, 2, 3, 4]\n",
    "# inicializamos los vectores de resultados-----------------------\n",
    "resultados_test = []\n",
    "# bucle para extraer resultados----------------------------------\n",
    "for eta in learning_rates:\n",
    "  for d in max_depths:\n",
    "   gb_clf_i = GradientBoostingClassifier(learning_rate = eta, \n",
    "   n_estimators = 500, random_state = 3, max_depth = d);\n",
    "   # entrenamos al modelo----------------------------------------\n",
    "   gb_clf_i.fit(X_train, y_train);\n",
    "   # prediccion sobre la muestra de validacion-------------------   \n",
    "   y_pred_test = gb_clf_i.predict(X_test);\n",
    "   # accuracy de entrenamiento-----------------------------------\n",
    "   acc_test = accuracy_score(y_test, y_pred_test);\n",
    "   # guardar resultados en el vector-----------------------------\n",
    "   resultados_test.append(acc_test);\n",
    "# resultados_test[0:6] # max_depth = 1\n",
    "# resultados_test[6:12] # max_depth = 2\n",
    "# resultados_test[12:18] # max_depth = 3\n",
    "# resultados_test[18:24] # max_depth = 4\n",
    "# learning_rates\n",
    "\n",
    "# pintamos los resultados de entrenamiento y validacion----------   \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "plt.figure(figsize=(8, 4.75))\n",
    "\n",
    "\n",
    "line1, = plt.plot(learning_rates, resultados_test[0:6], \"b\",\n",
    "label = \"max_depth = 1\")\n",
    "line2, = plt.plot(learning_rates, resultados_test[6:12], \"r\",\n",
    "label = \"max_depth = 2\")\n",
    "line2, = plt.plot(learning_rates, resultados_test[12:18], \"g\",\n",
    "label = \"max_depth = 3\")\n",
    "line2, = plt.plot(learning_rates, resultados_test[18:24], \"y\",\n",
    "label = \"max_depth = 4\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints = 4)})\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"learning rate\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
