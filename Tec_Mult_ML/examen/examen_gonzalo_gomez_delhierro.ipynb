{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c56c04-792e-41c9-ad42-7c25abaa4d0e",
   "metadata": {},
   "source": [
    "# EXAMEN - GONZALO MIGUEL GÓMEZ DEL HIERRO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323363de-d6ee-4731-9004-bff5ddb4ae32",
   "metadata": {},
   "source": [
    "En primer lugar vamos a generar los parámetros que se van a informar para la creación del dataset (que se llevará a cabo mediante la función make_regression de sklearn). Mi fecha de nacimiento es el 9 de junio de 1999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "56d9878e-e57c-4426-a162-e94fdde86512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importación de librerías\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d83cc6c5-1b68-496d-9c0b-cc15bd4a2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos lo parámetros tal como se describe en el enunciado (09/06/1999)\n",
    "m = 9+6\n",
    "s1 = 9\n",
    "# comento la siguiente línea para que no se rompa la coherencia al reejecutar\n",
    "# s2 = np.random.randint(1,5) \n",
    "s2 = 4 # valor inicial obtenido, incluido para poder reejecutar sin romper el notebook\n",
    "a=1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "056a8047-ae3b-4bf5-8d9e-2c8677372242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2 = 4\n"
     ]
    }
   ],
   "source": [
    "print('s2 = {}'.format(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2d7b1808-d2f0-40ee-b053-23db800c3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos el conjunto de datos \n",
    "X, y = make_regression(n_samples=500+m,n_features=10+s1+s2,n_informative=10+s1,noise=10*s1,bias=2,shuffle=False,random_state=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5f4e3-2a75-4149-b5b5-50fd452bf0a7",
   "metadata": {},
   "source": [
    "De esta manera, hemos generado una muestra con m observaciones (n_samples) compuestas por 10+s1+s2 variables independientes (n_features) y una variable objetivo. De estas 10+s1+s2 variables independientes, s2 son redundantes (n_features - n_informative). El parámetro noise se corresponde con la desviación estándar del ruido que se aplica sobre la muestra perfectamente lineal, para romper dicha linealidad. Como también se puede ver en la documentación, el parámetro bias es la ordenada en el origen del modelo lineal del que se parte. Es decir, si el parámetro noise fuera 0, este bias sería la constante ($\\beta_0$) en el modelo lineal $y = \\beta_0 + \\beta_1\\cdot x_1 \\cdot ... \\cdot \\beta_{10+s_1+s_2} \\cdot x_{10+s_1+s_2}$. Se define el parámetro shuffle como False para que no se \"barajen\" observaciones y características."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c4e2d-4258-490d-ac93-0f0fdf32eb03",
   "metadata": {},
   "source": [
    "Para poder ver de manera cómoda la información del conjunto de datos creado, vamos a integrarlo todo en un dataframe de la librería Pandas, que nos permite emplear directamente algunos métodos útiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ccdd2c0a-879e-43bc-9984-90eec1c46372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.317480</td>\n",
       "      <td>0.692062</td>\n",
       "      <td>-1.284378</td>\n",
       "      <td>0.393346</td>\n",
       "      <td>0.211803</td>\n",
       "      <td>-0.411457</td>\n",
       "      <td>0.945473</td>\n",
       "      <td>0.906433</td>\n",
       "      <td>-0.541158</td>\n",
       "      <td>1.204638</td>\n",
       "      <td>...</td>\n",
       "      <td>1.353890</td>\n",
       "      <td>0.804267</td>\n",
       "      <td>-0.355395</td>\n",
       "      <td>-0.804633</td>\n",
       "      <td>0.247044</td>\n",
       "      <td>-0.140479</td>\n",
       "      <td>-0.072590</td>\n",
       "      <td>-1.311976</td>\n",
       "      <td>-1.836959</td>\n",
       "      <td>101.593606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.241581</td>\n",
       "      <td>-0.830838</td>\n",
       "      <td>-1.377936</td>\n",
       "      <td>-0.096528</td>\n",
       "      <td>0.094396</td>\n",
       "      <td>-0.506670</td>\n",
       "      <td>0.624220</td>\n",
       "      <td>0.375006</td>\n",
       "      <td>0.852429</td>\n",
       "      <td>-0.633999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415251</td>\n",
       "      <td>-1.259956</td>\n",
       "      <td>-0.227572</td>\n",
       "      <td>0.404326</td>\n",
       "      <td>0.127519</td>\n",
       "      <td>-0.100616</td>\n",
       "      <td>0.194965</td>\n",
       "      <td>-1.533411</td>\n",
       "      <td>-0.601231</td>\n",
       "      <td>106.446044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.280362</td>\n",
       "      <td>-0.066893</td>\n",
       "      <td>0.254026</td>\n",
       "      <td>1.065242</td>\n",
       "      <td>-1.791752</td>\n",
       "      <td>1.583481</td>\n",
       "      <td>0.110303</td>\n",
       "      <td>-0.028591</td>\n",
       "      <td>0.406122</td>\n",
       "      <td>1.686036</td>\n",
       "      <td>...</td>\n",
       "      <td>1.281054</td>\n",
       "      <td>0.256899</td>\n",
       "      <td>0.942982</td>\n",
       "      <td>-2.221908</td>\n",
       "      <td>-0.385831</td>\n",
       "      <td>0.369260</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>-0.895873</td>\n",
       "      <td>-1.707936</td>\n",
       "      <td>332.887337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.510309</td>\n",
       "      <td>-0.491197</td>\n",
       "      <td>1.574700</td>\n",
       "      <td>0.668085</td>\n",
       "      <td>-0.470858</td>\n",
       "      <td>-0.052428</td>\n",
       "      <td>-0.642659</td>\n",
       "      <td>-0.877692</td>\n",
       "      <td>-0.664727</td>\n",
       "      <td>1.470971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.809129</td>\n",
       "      <td>-0.963956</td>\n",
       "      <td>-1.794985</td>\n",
       "      <td>-0.700473</td>\n",
       "      <td>-0.811009</td>\n",
       "      <td>-0.009502</td>\n",
       "      <td>-0.299622</td>\n",
       "      <td>2.093173</td>\n",
       "      <td>-0.740827</td>\n",
       "      <td>-58.012980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069464</td>\n",
       "      <td>0.363515</td>\n",
       "      <td>0.143828</td>\n",
       "      <td>-1.600201</td>\n",
       "      <td>0.862060</td>\n",
       "      <td>0.871241</td>\n",
       "      <td>0.397860</td>\n",
       "      <td>-1.021002</td>\n",
       "      <td>-0.512252</td>\n",
       "      <td>-0.756797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866779</td>\n",
       "      <td>1.302976</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>-0.129024</td>\n",
       "      <td>0.690126</td>\n",
       "      <td>-0.230427</td>\n",
       "      <td>0.006052</td>\n",
       "      <td>0.464079</td>\n",
       "      <td>-0.104459</td>\n",
       "      <td>-175.145887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>1.462845</td>\n",
       "      <td>-1.326999</td>\n",
       "      <td>0.292287</td>\n",
       "      <td>-0.287487</td>\n",
       "      <td>0.691148</td>\n",
       "      <td>1.729091</td>\n",
       "      <td>-0.104531</td>\n",
       "      <td>-2.334862</td>\n",
       "      <td>1.680202</td>\n",
       "      <td>0.800874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117482</td>\n",
       "      <td>-0.392527</td>\n",
       "      <td>0.168543</td>\n",
       "      <td>-1.195768</td>\n",
       "      <td>1.160966</td>\n",
       "      <td>-0.658549</td>\n",
       "      <td>0.084653</td>\n",
       "      <td>1.584790</td>\n",
       "      <td>-0.407782</td>\n",
       "      <td>-107.706031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0.776313</td>\n",
       "      <td>-1.131815</td>\n",
       "      <td>0.480638</td>\n",
       "      <td>-0.386624</td>\n",
       "      <td>0.819782</td>\n",
       "      <td>1.977061</td>\n",
       "      <td>-0.912984</td>\n",
       "      <td>0.414402</td>\n",
       "      <td>1.338057</td>\n",
       "      <td>2.101512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613059</td>\n",
       "      <td>-0.548573</td>\n",
       "      <td>-0.578250</td>\n",
       "      <td>-0.215159</td>\n",
       "      <td>-0.187473</td>\n",
       "      <td>0.560881</td>\n",
       "      <td>0.198613</td>\n",
       "      <td>-0.146773</td>\n",
       "      <td>-1.592810</td>\n",
       "      <td>37.297444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>-0.163110</td>\n",
       "      <td>-0.520379</td>\n",
       "      <td>-1.035334</td>\n",
       "      <td>-0.586096</td>\n",
       "      <td>1.011395</td>\n",
       "      <td>0.179459</td>\n",
       "      <td>0.577972</td>\n",
       "      <td>-0.163274</td>\n",
       "      <td>-0.968416</td>\n",
       "      <td>0.213152</td>\n",
       "      <td>...</td>\n",
       "      <td>1.174064</td>\n",
       "      <td>-1.569103</td>\n",
       "      <td>-0.619830</td>\n",
       "      <td>-0.015942</td>\n",
       "      <td>0.018698</td>\n",
       "      <td>-1.162886</td>\n",
       "      <td>0.747527</td>\n",
       "      <td>1.404390</td>\n",
       "      <td>1.340463</td>\n",
       "      <td>15.066860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.924158</td>\n",
       "      <td>0.582225</td>\n",
       "      <td>-1.781251</td>\n",
       "      <td>1.066854</td>\n",
       "      <td>1.403413</td>\n",
       "      <td>-0.241470</td>\n",
       "      <td>-0.870471</td>\n",
       "      <td>-0.303264</td>\n",
       "      <td>-2.101935</td>\n",
       "      <td>1.513839</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.694500</td>\n",
       "      <td>-0.176298</td>\n",
       "      <td>0.706053</td>\n",
       "      <td>2.592830</td>\n",
       "      <td>-0.495021</td>\n",
       "      <td>-0.234607</td>\n",
       "      <td>0.306296</td>\n",
       "      <td>-0.137493</td>\n",
       "      <td>0.630081</td>\n",
       "      <td>-61.015762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>-0.260618</td>\n",
       "      <td>0.491094</td>\n",
       "      <td>-0.182689</td>\n",
       "      <td>-0.516884</td>\n",
       "      <td>-0.565441</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.769098</td>\n",
       "      <td>-0.182311</td>\n",
       "      <td>-1.014352</td>\n",
       "      <td>0.938103</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.648536</td>\n",
       "      <td>0.120753</td>\n",
       "      <td>-0.349634</td>\n",
       "      <td>-0.818685</td>\n",
       "      <td>-0.519779</td>\n",
       "      <td>1.047659</td>\n",
       "      <td>-0.112029</td>\n",
       "      <td>0.164714</td>\n",
       "      <td>2.972027</td>\n",
       "      <td>-15.401570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>515 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0   -0.317480  0.692062 -1.284378  0.393346  0.211803 -0.411457  0.945473   \n",
       "1   -0.241581 -0.830838 -1.377936 -0.096528  0.094396 -0.506670  0.624220   \n",
       "2    1.280362 -0.066893  0.254026  1.065242 -1.791752  1.583481  0.110303   \n",
       "3    0.510309 -0.491197  1.574700  0.668085 -0.470858 -0.052428 -0.642659   \n",
       "4    0.069464  0.363515  0.143828 -1.600201  0.862060  0.871241  0.397860   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "510  1.462845 -1.326999  0.292287 -0.287487  0.691148  1.729091 -0.104531   \n",
       "511  0.776313 -1.131815  0.480638 -0.386624  0.819782  1.977061 -0.912984   \n",
       "512 -0.163110 -0.520379 -1.035334 -0.586096  1.011395  0.179459  0.577972   \n",
       "513  0.924158  0.582225 -1.781251  1.066854  1.403413 -0.241470 -0.870471   \n",
       "514 -0.260618  0.491094 -0.182689 -0.516884 -0.565441  0.413043  0.769098   \n",
       "\n",
       "           x7        x8        x9  ...       x14       x15       x16  \\\n",
       "0    0.906433 -0.541158  1.204638  ...  1.353890  0.804267 -0.355395   \n",
       "1    0.375006  0.852429 -0.633999  ...  0.415251 -1.259956 -0.227572   \n",
       "2   -0.028591  0.406122  1.686036  ...  1.281054  0.256899  0.942982   \n",
       "3   -0.877692 -0.664727  1.470971  ... -0.809129 -0.963956 -1.794985   \n",
       "4   -1.021002 -0.512252 -0.756797  ... -0.866779  1.302976  0.000415   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "510 -2.334862  1.680202  0.800874  ...  0.117482 -0.392527  0.168543   \n",
       "511  0.414402  1.338057  2.101512  ... -0.613059 -0.548573 -0.578250   \n",
       "512 -0.163274 -0.968416  0.213152  ...  1.174064 -1.569103 -0.619830   \n",
       "513 -0.303264 -2.101935  1.513839  ... -1.694500 -0.176298  0.706053   \n",
       "514 -0.182311 -1.014352  0.938103  ... -1.648536  0.120753 -0.349634   \n",
       "\n",
       "          x17       x18       x19       x20       x21       x22           y  \n",
       "0   -0.804633  0.247044 -0.140479 -0.072590 -1.311976 -1.836959  101.593606  \n",
       "1    0.404326  0.127519 -0.100616  0.194965 -1.533411 -0.601231  106.446044  \n",
       "2   -2.221908 -0.385831  0.369260  0.003005 -0.895873 -1.707936  332.887337  \n",
       "3   -0.700473 -0.811009 -0.009502 -0.299622  2.093173 -0.740827  -58.012980  \n",
       "4   -0.129024  0.690126 -0.230427  0.006052  0.464079 -0.104459 -175.145887  \n",
       "..        ...       ...       ...       ...       ...       ...         ...  \n",
       "510 -1.195768  1.160966 -0.658549  0.084653  1.584790 -0.407782 -107.706031  \n",
       "511 -0.215159 -0.187473  0.560881  0.198613 -0.146773 -1.592810   37.297444  \n",
       "512 -0.015942  0.018698 -1.162886  0.747527  1.404390  1.340463   15.066860  \n",
       "513  2.592830 -0.495021 -0.234607  0.306296 -0.137493  0.630081  -61.015762  \n",
       "514 -0.818685 -0.519779  1.047659 -0.112029  0.164714  2.972027  -15.401570  \n",
       "\n",
       "[515 rows x 24 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# etiquetas para las n_features variables independientes + la variable independiente\n",
    "columns = ['x{}'.format(i) for i in range(X.shape[1])]\n",
    "columns.append('y')\n",
    "# generamos un dataframe apropiado\n",
    "df = pd.DataFrame(np.hstack((X,y[:, None])), columns=columns)\n",
    "\n",
    "# describimos nuestro conjunto de datos con algunas funciones útiles\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9173eddc-d6d1-44c8-b829-183b50970f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 515 entries, 0 to 514\n",
      "Data columns (total 24 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   x0      515 non-null    float64\n",
      " 1   x1      515 non-null    float64\n",
      " 2   x2      515 non-null    float64\n",
      " 3   x3      515 non-null    float64\n",
      " 4   x4      515 non-null    float64\n",
      " 5   x5      515 non-null    float64\n",
      " 6   x6      515 non-null    float64\n",
      " 7   x7      515 non-null    float64\n",
      " 8   x8      515 non-null    float64\n",
      " 9   x9      515 non-null    float64\n",
      " 10  x10     515 non-null    float64\n",
      " 11  x11     515 non-null    float64\n",
      " 12  x12     515 non-null    float64\n",
      " 13  x13     515 non-null    float64\n",
      " 14  x14     515 non-null    float64\n",
      " 15  x15     515 non-null    float64\n",
      " 16  x16     515 non-null    float64\n",
      " 17  x17     515 non-null    float64\n",
      " 18  x18     515 non-null    float64\n",
      " 19  x19     515 non-null    float64\n",
      " 20  x20     515 non-null    float64\n",
      " 21  x21     515 non-null    float64\n",
      " 22  x22     515 non-null    float64\n",
      " 23  y       515 non-null    float64\n",
      "dtypes: float64(24)\n",
      "memory usage: 96.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44afaf4a-55a5-49b5-9031-0adc4119e228",
   "metadata": {},
   "source": [
    "Vemos que tenemos 23 variables independientes (10+s1+s2) y una variable dependiente (y). Además vemos que el número de observaciones (n_samples) es 515 (nuestro parámetro m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a7ec00dd-dae7-4963-b8fe-159b13480eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>515.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.031916</td>\n",
       "      <td>-0.018579</td>\n",
       "      <td>-0.063151</td>\n",
       "      <td>-0.021909</td>\n",
       "      <td>0.013476</td>\n",
       "      <td>-0.001181</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>-0.077559</td>\n",
       "      <td>-0.027426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005448</td>\n",
       "      <td>-0.014250</td>\n",
       "      <td>0.019805</td>\n",
       "      <td>0.063264</td>\n",
       "      <td>0.127322</td>\n",
       "      <td>0.011155</td>\n",
       "      <td>0.039549</td>\n",
       "      <td>0.022316</td>\n",
       "      <td>0.015988</td>\n",
       "      <td>9.216541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.986544</td>\n",
       "      <td>1.017873</td>\n",
       "      <td>0.954387</td>\n",
       "      <td>1.012893</td>\n",
       "      <td>0.991497</td>\n",
       "      <td>0.981665</td>\n",
       "      <td>0.981380</td>\n",
       "      <td>0.990812</td>\n",
       "      <td>1.005245</td>\n",
       "      <td>0.975874</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019443</td>\n",
       "      <td>0.966516</td>\n",
       "      <td>0.969294</td>\n",
       "      <td>0.998064</td>\n",
       "      <td>0.962973</td>\n",
       "      <td>0.966104</td>\n",
       "      <td>0.956670</td>\n",
       "      <td>0.981450</td>\n",
       "      <td>0.996918</td>\n",
       "      <td>236.614468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.779419</td>\n",
       "      <td>-3.658909</td>\n",
       "      <td>-3.210323</td>\n",
       "      <td>-3.109916</td>\n",
       "      <td>-2.474327</td>\n",
       "      <td>-3.026705</td>\n",
       "      <td>-3.537120</td>\n",
       "      <td>-2.962460</td>\n",
       "      <td>-2.665844</td>\n",
       "      <td>-2.676123</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.427870</td>\n",
       "      <td>-3.565831</td>\n",
       "      <td>-2.770447</td>\n",
       "      <td>-3.073767</td>\n",
       "      <td>-2.723248</td>\n",
       "      <td>-2.386994</td>\n",
       "      <td>-2.768433</td>\n",
       "      <td>-3.209209</td>\n",
       "      <td>-2.747800</td>\n",
       "      <td>-685.839868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.711398</td>\n",
       "      <td>-0.702240</td>\n",
       "      <td>-0.704915</td>\n",
       "      <td>-0.687428</td>\n",
       "      <td>-0.684554</td>\n",
       "      <td>-0.637186</td>\n",
       "      <td>-0.651610</td>\n",
       "      <td>-0.671965</td>\n",
       "      <td>-0.749489</td>\n",
       "      <td>-0.697305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.691434</td>\n",
       "      <td>-0.621313</td>\n",
       "      <td>-0.593593</td>\n",
       "      <td>-0.590866</td>\n",
       "      <td>-0.564741</td>\n",
       "      <td>-0.656772</td>\n",
       "      <td>-0.587554</td>\n",
       "      <td>-0.606704</td>\n",
       "      <td>-0.600697</td>\n",
       "      <td>-152.987811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.018907</td>\n",
       "      <td>-0.110463</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>-0.031170</td>\n",
       "      <td>0.068542</td>\n",
       "      <td>-0.031397</td>\n",
       "      <td>0.039952</td>\n",
       "      <td>0.026018</td>\n",
       "      <td>-0.095659</td>\n",
       "      <td>-0.019338</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053982</td>\n",
       "      <td>0.043549</td>\n",
       "      <td>0.054638</td>\n",
       "      <td>0.045501</td>\n",
       "      <td>0.145512</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.032970</td>\n",
       "      <td>-0.012728</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>-7.880606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.650191</td>\n",
       "      <td>0.623574</td>\n",
       "      <td>0.539673</td>\n",
       "      <td>0.656907</td>\n",
       "      <td>0.704536</td>\n",
       "      <td>0.649443</td>\n",
       "      <td>0.704193</td>\n",
       "      <td>0.712172</td>\n",
       "      <td>0.573096</td>\n",
       "      <td>0.631577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.636121</td>\n",
       "      <td>0.682097</td>\n",
       "      <td>0.745959</td>\n",
       "      <td>0.767340</td>\n",
       "      <td>0.625252</td>\n",
       "      <td>0.650687</td>\n",
       "      <td>0.718792</td>\n",
       "      <td>0.671854</td>\n",
       "      <td>160.917009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.014268</td>\n",
       "      <td>3.587054</td>\n",
       "      <td>2.776868</td>\n",
       "      <td>2.810854</td>\n",
       "      <td>3.348492</td>\n",
       "      <td>3.153710</td>\n",
       "      <td>2.622348</td>\n",
       "      <td>3.181456</td>\n",
       "      <td>3.879336</td>\n",
       "      <td>2.582256</td>\n",
       "      <td>...</td>\n",
       "      <td>3.850762</td>\n",
       "      <td>2.620665</td>\n",
       "      <td>2.707830</td>\n",
       "      <td>2.916495</td>\n",
       "      <td>3.023495</td>\n",
       "      <td>2.972084</td>\n",
       "      <td>3.258235</td>\n",
       "      <td>2.799680</td>\n",
       "      <td>3.435305</td>\n",
       "      <td>601.698464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               x0          x1          x2          x3          x4          x5  \\\n",
       "count  515.000000  515.000000  515.000000  515.000000  515.000000  515.000000   \n",
       "mean    -0.031916   -0.018579   -0.063151   -0.021909    0.013476   -0.001181   \n",
       "std      0.986544    1.017873    0.954387    1.012893    0.991497    0.981665   \n",
       "min     -3.779419   -3.658909   -3.210323   -3.109916   -2.474327   -3.026705   \n",
       "25%     -0.711398   -0.702240   -0.704915   -0.687428   -0.684554   -0.637186   \n",
       "50%      0.018907   -0.110463    0.004862   -0.031170    0.068542   -0.031397   \n",
       "75%      0.650191    0.623574    0.539673    0.656907    0.704536    0.649443   \n",
       "max      3.014268    3.587054    2.776868    2.810854    3.348492    3.153710   \n",
       "\n",
       "               x6          x7          x8          x9  ...         x14  \\\n",
       "count  515.000000  515.000000  515.000000  515.000000  ...  515.000000   \n",
       "mean     0.026900    0.013158   -0.077559   -0.027426  ...   -0.005448   \n",
       "std      0.981380    0.990812    1.005245    0.975874  ...    1.019443   \n",
       "min     -3.537120   -2.962460   -2.665844   -2.676123  ...   -3.427870   \n",
       "25%     -0.651610   -0.671965   -0.749489   -0.697305  ...   -0.691434   \n",
       "50%      0.039952    0.026018   -0.095659   -0.019338  ...   -0.053982   \n",
       "75%      0.704193    0.712172    0.573096    0.631577  ...    0.663351   \n",
       "max      2.622348    3.181456    3.879336    2.582256  ...    3.850762   \n",
       "\n",
       "              x15         x16         x17         x18         x19         x20  \\\n",
       "count  515.000000  515.000000  515.000000  515.000000  515.000000  515.000000   \n",
       "mean    -0.014250    0.019805    0.063264    0.127322    0.011155    0.039549   \n",
       "std      0.966516    0.969294    0.998064    0.962973    0.966104    0.956670   \n",
       "min     -3.565831   -2.770447   -3.073767   -2.723248   -2.386994   -2.768433   \n",
       "25%     -0.621313   -0.593593   -0.590866   -0.564741   -0.656772   -0.587554   \n",
       "50%      0.043549    0.054638    0.045501    0.145512    0.028615    0.032970   \n",
       "75%      0.636121    0.682097    0.745959    0.767340    0.625252    0.650687   \n",
       "max      2.620665    2.707830    2.916495    3.023495    2.972084    3.258235   \n",
       "\n",
       "              x21         x22           y  \n",
       "count  515.000000  515.000000  515.000000  \n",
       "mean     0.022316    0.015988    9.216541  \n",
       "std      0.981450    0.996918  236.614468  \n",
       "min     -3.209209   -2.747800 -685.839868  \n",
       "25%     -0.606704   -0.600697 -152.987811  \n",
       "50%     -0.012728    0.022261   -7.880606  \n",
       "75%      0.718792    0.671854  160.917009  \n",
       "max      2.799680    3.435305  601.698464  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff5450-af7d-4770-a86d-3a5ece9651f1",
   "metadata": {},
   "source": [
    "Veamos en una representación gráfica el índice de correlación entre las variables independientes y dependiente, con el objetivo de hacernos una primera idea de qué variables pueden ser más colineales con otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "318ebd0e-d3af-4a2c-b129-5f2770517045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Matriz de correlación\n",
    "corr = df.corr(method='pearson')\n",
    "# Mapa de calor sobre las correlaciones entre variables\n",
    "sns.heatmap(corr, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72549c49-8a2a-426f-9ce7-b7be218d8700",
   "metadata": {},
   "source": [
    "Vemos que es en las posiciones correspondientes a la correlación de caada variable independiente con la variable y donde vemos los colores que indican más correlación. Sin embargo, sí que se observan entre las últimas variables independientes colores que indican correlación baja con la variable dependiente.\n",
    "\n",
    "A continuación, vamos a representar los pares $(x_i,y)$ para los 23 valores de i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f57e8-0f12-415a-b565-a2b2dcbf8f95",
   "metadata": {},
   "source": [
    "sns.pairplot(df, x_vars=columns[0:-1], y_vars=columns[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff021fe5-de12-4523-affc-4e3342fc6888",
   "metadata": {},
   "source": [
    "El gráfico anterior no es muy sugerente. Si atendemos una a una a todas las representaciones, si podemos ver algunas en que la nube de puntos puede parecer más situada alrededor de una posible recta (por ejemplo x12) y otras en las que el desorden parece mayor y no se intuye un trasfondo lineal (como x19)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97124c-bda9-4a8d-bd42-886b7835cea9",
   "metadata": {},
   "source": [
    "A continuación, se pide dividir el dataset en entrenamiento (70%) y test(30%). Para ello, hacemos uso de la función train_test_split de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5b015d92-f7c2-4572-aef8-4ab493649727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=a) # si le pasamos un número decimal \n",
    "                                                                                          # en test_size lo entiende como porcentaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc51e1-b300-4791-a356-670f9741aa34",
   "metadata": {},
   "source": [
    "A continuación se pide realizar una regresión lineal multivariante sobre el conjunto de datos. Para ello, se utiliza la clase LinearRegression de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "94152461-b7d5-4b89-a545-0c703974d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Regresión habitual por mínimos cuadrados\n",
    "reg_ls = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b75b7b-32ac-4685-b151-6fa806c31db9",
   "metadata": {},
   "source": [
    "Podemos ver los coeficientes ajustados a los datos en el modelo de regresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3b405c9f-034b-4d05-94e5-25db48b317cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: \n",
      " [87.92759092 -0.73802934 29.4970399  56.75468674 38.96952379 21.59483232\n",
      " 56.99540147 96.2678824  32.92486733  0.50481603 36.080372    1.5135127\n",
      " 72.7645182   5.57702945 75.39416944 77.40141926 53.12179828 15.43438997\n",
      " 23.0921661  -0.38633887 -4.70628473  5.93119733  2.61292669] \n",
      " Intercepto: \n",
      " 5.7304810656361775\n"
     ]
    }
   ],
   "source": [
    "# Coeficientes del modelo\n",
    "print(\"Coeficientes: \\n\", reg_ls.coef_, \"\\n\", \"Intercepto: \\n\", reg_ls.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a664735-8574-4fa3-b54d-c7d166eee93a",
   "metadata": {},
   "source": [
    "Vemos que el peso de algunas variables es significativamente menor al de otras. A continuación, evaluamos la bondad de nuestro ajuste hciendo uso de la meustra de test y de algunas funciones propias de sklearn. Recordemos que el bias=2 significaba que en el caso en el que el parámetro noise fuera nulo, la ordenada en el origen del modelo lineal generado sería ese valor del sesgo (2 en este caso). Vemos que el intercepto (que es esa ordenada en el origen) es ahora 5.73, lo cual es una consecuencia de la \"cantidad\" de ruido introducido en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eabd3447-f920-437c-8ec4-bcd9491388e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cuadrático medio: 8174.36\n",
      "Coeficiente de determinación R2: 0.84\n"
     ]
    }
   ],
   "source": [
    "# evalúamos la función obtenida en la muestra de variables independientes reservada para test\n",
    "y_pred_ls = reg_ls.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Error cuadrático medio sobre la muestra de test\n",
    "mean_error_ls = mean_squared_error(y_test, y_pred_ls)\n",
    "print(\"Error cuadrático medio: %.2f\" % mean_error_ls)\n",
    "# Coeficiente de determinación 1 significa predicción perfecta\n",
    "r2_ls = r2_score(y_test, y_pred_ls)\n",
    "print(\"Coeficiente de determinación R2: %.2f\" % r2_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f14d60-a755-4297-b265-88e482f4ec97",
   "metadata": {},
   "source": [
    "Vemos que el coeficiente de determinación $R^2$, siendo bueno, no es exageradamente bueno, lo que se podía esperar teniendo en cuenta el valor del parámetro noise (90) usado al generar el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f46289-a3be-4aef-9c82-bb891cfd71fd",
   "metadata": {},
   "source": [
    "A continuación, veamos la normalidad y homocedasticidad de los residuos para validar lo razonable del ajuste lineal sobre el dataset en estudio \n",
    "(deberían superarse todos los test puesto que la muestra esta construida a medida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6f1ce2dd-395a-4d3b-abb9-b9e13b6d2f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Shapiro-Wilk, p.valor: 0.57584\n",
      "Como p.valor > 0.05, no se rechaza la hipótesis nula y se da normalidad en los residuos.\n"
     ]
    }
   ],
   "source": [
    "# calculamos los residuos sobre la muestra de entrenamiento\n",
    "y_train_ls = reg_ls.predict(X_train)\n",
    "residuos = y_train - y_train_ls\n",
    "# Test de Saphiro-Wilk para probar la normalidad de los residuos (por ser una muestra pequeña <5000 observaciones)\n",
    "import scipy.stats as stats\n",
    "sh_result = stats.shapiro(residuos)\n",
    "print(\"Test Shapiro-Wilk, p.valor: %5.5f\" %(sh_result.pvalue))\n",
    "print(\"Como p.valor > 0.05, no se rechaza la hipótesis nula y se da normalidad en los residuos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec6d8d9-8683-4a25-8248-b302644ac6bd",
   "metadata": {},
   "source": [
    "Para llevar a cabo el test de Breusch-Pagan para corroborar la homocedasticidad, vamos a emplear la regresión lineal por mínimos cuadrados de alibrería statsmodels, que ya tiene algunos métodos preimplementados. Mostraremos para dar confort al lector que se obtienen los mismos resultados que con la regresión lineal de sklearn, y que por tanto los resultados obtenidos sobre una son igualmente válidos para la otra, aunque a nivel de código se trate de dos objetos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2d431fbe-ee7f-470b-96ea-7c43636865e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia en los interceptos: -5.773159728050814e-14\n",
      "Diferencia en los coeficientes:[ 2.27373675e-13  2.21156427e-13  7.10542736e-15  1.77635684e-13\n",
      " -2.06057393e-13  1.88293825e-13  1.56319402e-13  2.13162821e-13\n",
      " -3.26849658e-13 -4.97379915e-14  1.20792265e-13  2.20268248e-13\n",
      "  2.84217094e-14  1.39444012e-13  3.41060513e-13  1.13686838e-13\n",
      " -8.52651283e-14 -2.48689958e-14  2.84217094e-14  2.04281037e-14\n",
      "  7.28306304e-14  4.44089210e-14  3.10418358e-13]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "# forma de construir la regresión lineal \n",
    "X_train2 = sm.add_constant(X_train)\n",
    "reg_ls_stats = sm.OLS(y_train, X_train2).fit()\n",
    "# demostración de que se trata del mismo ajuste \n",
    "print('Diferencia en los interceptos: {}'.format(reg_ls_stats.params[0] - reg_ls.intercept_))\n",
    "print('Diferencia en los coeficientes:{}'.format(reg_ls_stats.params[1:] - reg_ls.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fdc68054-0887-42f9-b95d-381e46340c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El resultado del test Breusch-Pagan es: p.valor = 0.483\n",
      "Como p.valor > 0.05, no se rechaza la hipótesis nula y se da homocedasticidad.\n"
     ]
    }
   ],
   "source": [
    "# test de Breusch-Pagan\n",
    "import statsmodels.stats.api as sms\n",
    "bp = sms.het_breuschpagan(resid = reg_ls_stats.resid, exog_het = reg_ls_stats.model.exog)[1]\n",
    "print(\"El resultado del test Breusch-Pagan es: p.valor = %5.3f\"%(bp))\n",
    "print(\"Como p.valor > 0.05, no se rechaza la hipótesis nula y se da homocedasticidad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c3d41-5e7f-4771-b7a1-8c968afde49b",
   "metadata": {},
   "source": [
    "Tras los test hechos, podemos concluir que el modelo es una buena elección para ajustar los datos generados (evidentemente)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486f18f-b615-4dee-9811-eb7fca324a78",
   "metadata": {},
   "source": [
    "Realicemos el test ICbeta para poder concluir cuáles son las variables que no son significativas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d38182b8-9301-4b40-a4ef-9abf9f479de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El IC al 0.95 de b1 es: [87.92759092 -0.73802934 29.4970399  56.75468674 38.96952379 21.59483232\n",
      " 56.99540147 96.2678824  32.92486733  0.50481603 36.080372    1.5135127\n",
      " 72.7645182   5.57702945 75.39416944 77.40141926 53.12179828 15.43438997\n",
      " 23.0921661  -0.38633887 -4.70628473  5.93119733  2.61292669] +/- 7.970996172644631\n",
      "El intervalo de confianza para la variable x1 contiene al 0, luego no es significativa.\n",
      "El intervalo de confianza para la variable x9 contiene al 0, luego no es significativa.\n",
      "El intervalo de confianza para la variable x11 contiene al 0, luego no es significativa.\n",
      "El intervalo de confianza para la variable x13 contiene al 0, luego no es significativa.\n",
      "El intervalo de confianza para la variable x19 contiene al 0, luego no es significativa.\n",
      "El intervalo de confianza para la variable x20 contiene al 0, luego no es significativa.\n",
      "El intervalo de confianza para la variable x21 contiene al 0, luego no es significativa.\n",
      "El intervalo de confianza para la variable x22 contiene al 0, luego no es significativa.\n"
     ]
    }
   ],
   "source": [
    "# calcular ICbeta1\n",
    "\n",
    "# calcular numerador sb1^2\n",
    "s2 = sum(residuos**2)/(len(y_train)-2)\n",
    "# calcular denominador sb1^2\n",
    "den = np.var(X) * len(X)\n",
    "# calcular sb1\n",
    "sb1 = (s2/den) ** 0.5\n",
    "amplitud = 1.96 * sb1\n",
    "print(\"El IC al 0.95 de b1 es:\", reg_ls.coef_, \"+/-\", amplitud)\n",
    "for i in range(len(columns[0:-1])):\n",
    "    if reg_ls.coef_[i] - amplitud < 0 and reg_ls.coef_[i] + amplitud > 0:\n",
    "        print(\"El intervalo de confianza para la variable {} contiene al 0, luego no es significativa.\".format(columns[i]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9d20b-700d-4dc3-82e4-2e06e45ab1d0",
   "metadata": {},
   "source": [
    "Del análisis anterior obtenemos más variables que no son significativas a parte de las que ya habíamos introducido a través de los parámetros del modelo (s2=4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76d6df-0589-460c-93a5-918874c6303a",
   "metadata": {},
   "source": [
    "El objeto creado a partir de la librería de statsmodels tiene un método muy útil para poder sacar más conclusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5c9d8f45-302d-473d-bf50-11682ecf9aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.856\n",
      "Model:                            OLS   Adj. R-squared:                  0.847\n",
      "Method:                 Least Squares   F-statistic:                     87.14\n",
      "Date:                Sun, 14 Jul 2024   Prob (F-statistic):          1.35e-126\n",
      "Time:                        13:25:01   Log-Likelihood:                -2133.9\n",
      "No. Observations:                 360   AIC:                             4316.\n",
      "Df Residuals:                     336   BIC:                             4409.\n",
      "Df Model:                          23                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          5.7305      5.091      1.126      0.261      -4.284      15.745\n",
      "x1            87.9276      5.083     17.298      0.000      77.929      97.927\n",
      "x2            -0.7380      5.009     -0.147      0.883     -10.592       9.116\n",
      "x3            29.4970      5.388      5.474      0.000      18.898      40.096\n",
      "x4            56.7547      5.136     11.050      0.000      46.652      66.857\n",
      "x5            38.9695      5.055      7.709      0.000      29.025      48.914\n",
      "x6            21.5948      5.302      4.073      0.000      11.166      32.023\n",
      "x7            56.9954      5.301     10.752      0.000      46.568      67.423\n",
      "x8            96.2679      5.216     18.456      0.000      86.008     106.528\n",
      "x9            32.9249      5.027      6.549      0.000      23.036      42.814\n",
      "x10            0.5048      5.255      0.096      0.924      -9.832      10.842\n",
      "x11           36.0804      5.434      6.640      0.000      25.392      46.769\n",
      "x12            1.5135      5.473      0.277      0.782      -9.251      12.278\n",
      "x13           72.7645      5.215     13.953      0.000      62.507      83.022\n",
      "x14            5.5770      4.705      1.185      0.237      -3.677      14.831\n",
      "x15           75.3942      5.052     14.923      0.000      65.456      85.332\n",
      "x16           77.4014      5.299     14.607      0.000      66.978      87.824\n",
      "x17           53.1218      5.291     10.040      0.000      42.714      63.530\n",
      "x18           15.4344      5.102      3.025      0.003       5.399      25.470\n",
      "x19           23.0922      5.570      4.146      0.000      12.135      34.049\n",
      "x20           -0.3863      5.333     -0.072      0.942     -10.877      10.105\n",
      "x21           -4.7063      5.380     -0.875      0.382     -15.288       5.876\n",
      "x22            5.9312      5.307      1.118      0.265      -4.509      16.371\n",
      "x23            2.6129      5.432      0.481      0.631      -8.073      13.298\n",
      "==============================================================================\n",
      "Omnibus:                        0.801   Durbin-Watson:                   1.945\n",
      "Prob(Omnibus):                  0.670   Jarque-Bera (JB):                0.659\n",
      "Skew:                          -0.100   Prob(JB):                        0.719\n",
      "Kurtosis:                       3.061   Cond. No.                         1.65\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(reg_ls_stats.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2689a9-4a3f-416f-b109-59ec3ae4f480",
   "metadata": {},
   "source": [
    "Hay que ser cuidadoso al ver el resumen anterior, puesto que las etiquetas puestas a las variables independientes van de 1 a 23, mientras que las etiquetas que hemos venido usando iban de 0 a 22. Esto es, nuestra variable x0 se corresponde con la x1 en la tabla previa.\n",
    "Atendiendo a la columna P>|t| (mayor valor implica menor significatividad estadística) de nuevo podemos observarque las variables $x_{1}$, $x_{9}$, $x_{11}$, $x_{13}$, $x_{19}$, $x_{20}$,  $x_{21}$ y  $x_{22}$ son candidatas claras a ser redundantes.\n",
    "\n",
    "A continuación, vamos a realizar un algoritmo stepwise hacia atrás, en el que guiados por la variable con mayor p-valor, y tomando como condición que el R^2 del ajuste realizado no decrezca de manera muy exagerada entre 2 pasos, vamos a eliminar las variables que sean redundantes.\n",
    "En el método se imponen 2 condiciones: la primera es que para poder eliminar una variable la diferencia entre el máximo p-valor (variable que se eliminaría) y mínimo p-valor, tiene que ser suficientmente grande. Esto lo hago para evitar que se eliminen todas las variables. La segunda condición es que el cambio relativo en el $r^2$ de ajuste no sea muy grande, y por tanto que la eliminación de la variable no esté afectando de manera muy negativa a la capacidad predictora del modelo.\n",
    "\n",
    "De esta manera, eliminamos de manera justificada todas las variables que no son significativas en nuestro modelo, mediante un algoritmo stepwise hacia atrás de elaboración propia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ce44d53c-9ada-4a72-8d22-c9da018e6a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables eliminadas: ['x19', 'x9', 'x1', 'x11', 'x22', 'x20', 'x21', 'x13']\n",
      "Variables significativas: ['x0', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x10', 'x12', 'x14', 'x15', 'x16', 'x17', 'x18']\n"
     ]
    }
   ],
   "source": [
    "# construimos dataframes con las muestras de test y entrenamiento para poder manejar las columnas que vamos eliminando\n",
    "df_train = pd.DataFrame(np.hstack((X_train,y_train[:, None])), columns=columns)\n",
    "df_test = pd.DataFrame(np.hstack((X_test,y_test[:, None])), columns=columns)\n",
    "# comenzamos considerando todas las variables\n",
    "x_columns = columns[0:-1]\n",
    "# cogemos la variable con mayor p-valor, excluimos de la lista de p-valores el que corresponde al intercepto\n",
    "idx = np.argwhere(reg_ls_stats.pvalues[1:] == max(reg_ls_stats.pvalues[1:]))[0][0]\n",
    "# inicializo variables \n",
    "iter = 0\n",
    "removed_var = []\n",
    "tol = 1\n",
    "tol2 = 0\n",
    "r2_new = r2_ls\n",
    "while tol > 1e-2 and tol2 < 0.1 :    \n",
    "    iter += 1\n",
    "    removed_var.append(x_columns[idx])\n",
    "    x_columns.remove(x_columns[idx])    \n",
    "    X2 = sm.add_constant(df_train[x_columns])\n",
    "    est_aux = sm.OLS(y_train,X2).fit()\n",
    "    sk_aux = LinearRegression().fit(df_train[x_columns],y_train)\n",
    "    ypred_aux=sk_aux.predict(df_test[x_columns])\n",
    "    r2_old = cp.copy(r2_new)\n",
    "    r2_new = r2_score(y_test, ypred_aux)\n",
    "    tol2=(r2_old-r2_new)/r2_old\n",
    "    idx = np.argwhere(est_aux.pvalues[1:] == max(est_aux.pvalues[1:]))[0][0]\n",
    "    tol = abs(max(est_aux.pvalues[1:]) - min(est_aux.pvalues[1:]))\n",
    "print(\"Variables eliminadas:\", removed_var)\n",
    "print(\"Variables significativas:\", x_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a6ea01cb-c484-49d7-af18-a3bae7fddd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.855\n",
      "Model:                            OLS   Adj. R-squared:                  0.848\n",
      "Method:                 Least Squares   F-statistic:                     135.0\n",
      "Date:                Sun, 14 Jul 2024   Prob (F-statistic):          5.63e-134\n",
      "Time:                        13:25:01   Log-Likelihood:                -2136.0\n",
      "No. Observations:                 360   AIC:                             4304.\n",
      "Df Residuals:                     344   BIC:                             4366.\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          5.7309      5.006      1.145      0.253      -4.116      15.577\n",
      "x0            88.2650      5.015     17.602      0.000      78.402      98.128\n",
      "x2            29.5984      5.320      5.564      0.000      19.135      40.062\n",
      "x3            56.2765      5.035     11.176      0.000      46.372      66.181\n",
      "x4            38.8889      4.996      7.784      0.000      29.063      48.715\n",
      "x5            21.5564      5.217      4.132      0.000      11.296      31.817\n",
      "x6            57.5324      5.227     11.006      0.000      47.251      67.814\n",
      "x7            96.9679      5.104     18.998      0.000      86.929     107.007\n",
      "x8            34.2125      4.868      7.028      0.000      24.638      43.787\n",
      "x10           34.9551      5.329      6.560      0.000      24.474      45.436\n",
      "x12           72.6545      5.137     14.143      0.000      62.550      82.759\n",
      "x14           75.2863      4.958     15.185      0.000      65.535      85.038\n",
      "x15           77.5523      5.234     14.816      0.000      67.257      87.848\n",
      "x16           53.6890      5.131     10.465      0.000      43.598      63.780\n",
      "x17           15.8486      5.020      3.157      0.002       5.974      25.723\n",
      "x18           24.1634      5.432      4.448      0.000      13.479      34.847\n",
      "==============================================================================\n",
      "Omnibus:                        0.774   Durbin-Watson:                   1.938\n",
      "Prob(Omnibus):                  0.679   Jarque-Bera (JB):                0.572\n",
      "Skew:                          -0.077   Prob(JB):                        0.751\n",
      "Kurtosis:                       3.120   Cond. No.                         1.47\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# resumen del ajuste al que se ha convergido\n",
    "print(est_aux.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37db43-eb18-4ca7-aa31-15ceea7123fe",
   "metadata": {},
   "source": [
    "A continuación se pie realizar una regresión con el método de Lasso tomando $\\alpha=10^4$. Se va a llevar a cabo el ajuste teniendo en cuenta todas las variables, puesto que al momento de llegar a este apartado todavía no he refinado el método stepwise del apartado anterior que podría ser  apropiado enchufar a esta parte (solo considerando las variables significativas). Puesto que más información, aunque no sea significativa, no debería suponer un obstáculo en una regresión lineal, prefiero hacer esta parte considerando todas las variables. En el ajuste que se lleva a cabo abajo, podemos ver que sobre la muestra de entrenamiento y test generadas, Lasso (con el valor de $\\alpha$ escogido) se comporta mucho peor que la regresión por mínimos cuadrados que veníamos tratando. Habría que llevar a cabo un algoritmo más riguroso por validación cruzada para la elección de este $\\alpha$ si quisiéramos concluir que la regresión de Lasso no aporta en ningún caso ninguna mejora respecto a la regresión por mínimos cuadrados sobre el conjunto de datos que estamos estudiando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "94df023d-793f-442d-bdeb-d4d837560b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error cuadráctico medio cometido con la regresión de Lasso es: 10946.35904201991\n",
      "Coeficiente de determinación r2 con la regresión de Lasso: 0.785957875275447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "alpha=10^4\n",
    "reg_lasso = Lasso(alpha=alpha).fit(X_train, y_train)\n",
    "y_pred_lasso = reg_lasso.predict(X_test)\n",
    "lasso_mean_error = mean_squared_error(y_test,y_pred_lasso)\n",
    "lasso_r2 = r2_score(y_test,y_pred_lasso) \n",
    "print('El error cuadráctico medio cometido con la regresión de Lasso es: {}'.format(lasso_mean_error))\n",
    "print('Coeficiente de determinación r2 con la regresión de Lasso: {}'.format(lasso_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
